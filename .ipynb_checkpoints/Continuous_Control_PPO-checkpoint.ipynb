{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='./Reacher-20.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "\n",
    "class MeanStdNormalizer():\n",
    "    def __init__(self):\n",
    "        self.rms = None\n",
    "        self.clip = 10.0\n",
    "        self.epsilon = 1e-10\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1, ) + x.shape[1:])\n",
    "        self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)    \n",
    "\n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)    \n",
    "        \n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "def stack_tensor(some_list):\n",
    "    return torch.cat(some_list[:1000], dim=0)\n",
    "\n",
    "def tensor(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = torch.tensor(x, device=device, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def to_np(t):\n",
    "    return t.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class SubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, output_size, seed):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        dims = (input_size,) + hidden_units        \n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.output_layer = layer_init(nn.Linear(self.feature_dim, output_size), 1e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = self.output_layer(x)    \n",
    "        return x    \n",
    "            \n",
    "class ActorAndCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        super(ActorAndCritic, self).__init__()\n",
    "        self.seed = random.seed(seed)\n",
    "        self.actor = SubNetwork(state_size, (128, 128), action_size, seed)\n",
    "        self.critic = SubNetwork(state_size, (128, 128), 1, seed)\n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "        #self.to(Config.DEVICE)\n",
    "        \n",
    "    def forward(self, obs, action=None):\n",
    "        obs = torch.tensor(obs, dtype = torch.float)\n",
    "        a = self.actor(obs)\n",
    "        v = self.critic(obs)\n",
    "        mean = F.tanh(a)\n",
    "        dist = torch.distributions.Normal(mean, F.softplus(self.std))\n",
    "        return v, dist\n",
    "    \n",
    "    \n",
    "network = ActorAndCritic(20, 33, 4, 1)\n",
    "optimizer = optim.Adam(network.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(action_distribution):\n",
    "    actions = action_distribution.sample()\n",
    "    log_prob = get_log_prob(action_distribution, actions)\n",
    "    return actions\n",
    "\n",
    "def evaluate_actions_against_states(states, actions):\n",
    "    value, distribution = network(states, actions)\n",
    "    log_prob = get_log_prob(distribution, actions)\n",
    "    return value, log_prob\n",
    "    \n",
    "def get_log_prob(action_distribution, actions):\n",
    "    return action_distribution.log_prob(actions).mean(-1).unsqueeze(-1)\n",
    "    \n",
    "def loss_func(max_t, old_log_probs, actions, values, rewards, states, dones, epoch = 10, gae_lamda = 0.95, discount = 0.99, epsilon=0.2, beta=0.01):\n",
    "    discounts = discount ** torch.arange(max_t)\n",
    "    dones_num = torch.empty(max_t, 20)\n",
    "    for i in range(max_t):\n",
    "        for j in range(20):\n",
    "            if dones[i][j] == False:\n",
    "                dones_num[i, j] = 1\n",
    "            else:\n",
    "                dones_num[i, j] = 0\n",
    "                \n",
    "    advantage = torch.zeros((20, ))\n",
    "    advantages = [0.0] * 1000   \n",
    "    for i in reversed(range(max_t)):\n",
    "        if i == max_t - 1:\n",
    "            td = torch.tensor(rewards[i]) - values[i].squeeze(1)\n",
    "        else:\n",
    "            td = torch.tensor(rewards[i]) + (discount * dones_num[i] * values[i + 1].squeeze(1)) - values[i].squeeze(1)\n",
    "        advantage = advantage * gae_lamda * discount * dones_num[i] + td\n",
    "        advantages[i] = advantage.detach()\n",
    "            \n",
    "#     advantages = torch.flip(torch.tensor(advantages), dims = (0,)) * discounts.unsqueeze(1)\n",
    "#     advantages = advantages.cumsum(axis = 0).flip(dims = (0,))\n",
    "#     advantages = (advantages - advantages.mean())/(advantages.std())\n",
    "    \n",
    "    return_indiv = torch.zeros((20, ))\n",
    "    returns = [0.0] * 1000   \n",
    "    for i in reversed(range(max_t)):\n",
    "        return_indiv = torch.tensor(rewards[i]) + discount * dones_num[i] * return_indiv\n",
    "        returns[i] = return_indiv.detach() \n",
    "        \n",
    "#     returns = torch.flip(torch.tensor(returns), dims = (0,)) * discounts.unsqueeze(1)\n",
    "#     returns = returns.cumsum(axis = 0).flip(dims = (0,))\n",
    "#     returns = (returns - torch.mean(returns, dim = -1).unsqueeze(1))/(torch.std(returns, dim = -1).unsqueeze(1)+1e-10)\n",
    "    \n",
    "    states = stack_tensor(states)\n",
    "    actions = stack_tensor(actions)\n",
    "    old_log_probs = stack_tensor(old_log_probs)\n",
    "    returns = stack_tensor(returns)\n",
    "    advantages = stack_tensor(advantages)\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "#     old_log_probs = torch.stack(old_log_probs)\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        sampled_idxs = random_sample(np.arange(20 * 1000), 128)\n",
    "        for sampled_idx in sampled_idxs:\n",
    "            old_log_probs_1 = old_log_probs[sampled_idx]\n",
    "            states_1 = torch.tensor(states)[sampled_idx]\n",
    "            actions_1 = actions[sampled_idx]\n",
    "            values_1, new_log_probs_1 = evaluate_actions_against_states(states_1, actions_1)\n",
    "            advantages_1 = advantages[sampled_idx]\n",
    "            returns_1 = returns[sampled_idx]\n",
    "\n",
    "            old_log_probs_1 = old_log_probs_1.detach()\n",
    "            old_probs_1 = torch.exp(old_log_probs_1).mean(-1)\n",
    "            new_probs_1 = torch.exp(new_log_probs_1).mean(-1)\n",
    "            ratios = new_probs_1 / old_probs_1\n",
    "            ratios_alt = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "            entropy = -(new_probs_1 * torch.log(old_probs_1 + 1e-10) + (1.0 - new_probs_1) * torch.log(1.0 - old_probs_1 + 1e-10))\n",
    "            entropy[torch.isnan(entropy)] = 0\n",
    "            L_clipped = -torch.min(ratios * advantages_1, ratios_alt * advantages_1)\n",
    "            L_loss = torch.mean(L_clipped + beta * entropy)\n",
    "            loss = 0.5 * torch.mean((values_1.squeeze(1) - returns_1)**2)\n",
    "            total_loss = L_loss + loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), 1) \n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training(n_episodes=300, max_t=1000, discount = 0.99):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode = True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        score = np.zeros(20)\n",
    "        log_probs, states, actions, rewards, dones, values = [], [], [], [], [], []\n",
    "        for t in range(max_t):\n",
    "            value, dist = network(state)\n",
    "            action = get_prediction(dist)\n",
    "            value, log_prob = evaluate_actions_against_states(state, action)\n",
    "            env_info = env.step(action.detach().numpy())[brain_name]\n",
    "            next_state, reward, done = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            states.append(tensor(state))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            values.append(value)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            \n",
    "        loss_func(max_t, log_probs, actions, values, rewards, states, dones, epoch = 10, gae_lamda = 0.95, discount = 0.99, epsilon=0.2, beta=0.01)\n",
    "        \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)              \n",
    "        if i_episode % 5 == 0:\n",
    "            print('\\rEpisode {}\\tReward: {:.2f}\\tAverage Reward: {:.2f}'.format(i_episode, np.mean(score), np.mean(scores_window)))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.dqn.state_dict(), 'dqn.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
